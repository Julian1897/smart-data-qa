from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import sqlite3
import os
from typing import Dict, Any, List, Optional
import traceback
import tempfile
from pydantic import BaseModel
import json
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

app = FastAPI(title="æ™ºèƒ½æ•°æ®é—®ç­”ç³»ç»Ÿ", version="1.0.0")

# è®¾ç½®æœ€å¤§æ–‡ä»¶ä¸Šä¼ å¤§å°ä¸º100MB
app.max_request_size = 100 * 1024 * 1024

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:3001", "http://127.0.0.1:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class QueryRequest(BaseModel):
    question: str
    session_id: str
    conversation_id: Optional[str] = None

class DataSourceInfo(BaseModel):
    name: str
    type: str
    columns: List[str]
    row_count: int

class ModelConfig(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    api_key: str
    api_base: str
    model_name: str

# å­˜å‚¨ä¼šè¯ä¿¡æ¯
sessions = {}

# å­˜å‚¨å¯¹è¯å†å²
conversations = {}

# LLMé…ç½®
def create_llm(api_key: str = None, api_base: str = None, model_name: str = None):
    """åˆ›å»ºLLMå®ä¾‹ï¼Œæ”¯æŒè‡ªå®šä¹‰é…ç½®"""
    try:
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
        final_api_key = api_key or os.getenv("OPENAI_API_KEY")
        final_api_base = api_base or os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        final_model_name = model_name or os.getenv("MODEL_NAME", "gpt-3.5-turbo")
        
        if not final_api_key:
            raise ValueError("API Key is required")
        
        # åˆ›å»ºChatOpenAIå®ä¾‹ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯¼å…¥é˜¶æ®µå´©æºƒï¼‰
        from langchain_openai import ChatOpenAI  # å»¶è¿Ÿå¯¼å…¥

        llm = ChatOpenAI(
            openai_api_key=final_api_key,
            openai_api_base=final_api_base,
            model_name=final_model_name,
            temperature=0
        )
        return llm
    except Exception as e:
        print(f"LLMåˆ›å»ºå¤±è´¥: {e}")
        return None

# å°è¯•ä»¥å¤šç§å¸¸è§ç¼–ç è¯»å– CSV
def read_csv_with_auto_encoding(file_path: str) -> pd.DataFrame:
    candidate_encodings = [
        "utf-8",
        "utf-8-sig",
        "gbk",
        "gb2312",
        "big5",
        "shift_jis",
        "latin1",
    ]

    last_error: Optional[Exception] = None

    for enc in candidate_encodings:
        try:
            # å…ˆå°è¯•è‡ªåŠ¨åˆ†éš”ç¬¦ä¸ Python å¼•æ“ï¼ˆæ›´å®¹é”™ï¼‰ï¼Œå¹¶è·³è¿‡åè¡Œ
            return pd.read_csv(
                file_path,
                encoding=enc,
                sep=None,
                engine="python",
                on_bad_lines="skip",
            )
        except UnicodeDecodeError as e:
            last_error = e
            continue
        except Exception:
            # å†å°è¯•é»˜è®¤ C å¼•æ“çš„å¸¸è§„è¯»å–
            try:
                return pd.read_csv(
                    file_path,
                    encoding=enc,
                    on_bad_lines="skip",
                )
            except Exception as e2:
                last_error = e2
                continue

    # æœ€åå…œåº•ï¼šå¿½ç•¥æ— æ³•è§£ç çš„å­—èŠ‚ï¼ˆpandas>=2.0 æ”¯æŒ encoding_errorsï¼‰
    try:
        return pd.read_csv(
            file_path,
            encoding="utf-8",
            encoding_errors="ignore",
            sep=None,
            engine="python",
            on_bad_lines="skip",
        )
    except TypeError:
        # å¦‚æœä¸æ”¯æŒ encoding_errorsï¼Œåˆ™é€€å› latin1
        return pd.read_csv(
            file_path,
            encoding="latin1",
            sep=None,
            engine="python",
            on_bad_lines="skip",
        )
    except Exception as e:
        # å¦‚æœä¾ç„¶å¤±è´¥ï¼ŒæŠ›å‡ºæ›´æ˜ç¡®çš„å¼‚å¸¸
        raise HTTPException(status_code=400, detail=f"CSV ç¼–ç æ— æ³•è¯†åˆ«æˆ–è§£æå¤±è´¥: {str(last_error or e)}")

# å®‰å…¨è¯»å– Excelï¼Œé’ˆå¯¹ä¸åŒæ‰©å±•é€‰æ‹©å¼•æ“
def read_excel_with_engine(file_path: str, extension: str) -> pd.DataFrame:
    ext = extension.lower()
    try:
        if ext == ".xls":
            try:
                import xlrd  # noqa: F401
            except Exception:
                raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„æ˜¯æ—§ç‰ˆ Excel(XLS) æ–‡ä»¶ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–ï¼šxlrd==1.2.0")
            return pd.read_excel(file_path, engine="xlrd")
        else:
            # xlsx/xlsm/xltx/xltm ç­‰æ–°ç‰ˆ Excel
            return pd.read_excel(file_path, engine="openpyxl")
    except HTTPException:
        raise
    except ImportError as e:
        raise HTTPException(status_code=400, detail=f"è¯»å– Excel å¤±è´¥ï¼Œç¼ºå°‘ä¾èµ–ï¼š{str(e)}")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Excel è§£æå¤±è´¥ï¼š{str(e)}")

def extract_answer_from_full_response(full_response: str) -> str:
    """ä»SQl+ç­”æ¡ˆçš„å®Œæ•´å“åº”ä¸­æå–çº¯ç­”æ¡ˆéƒ¨åˆ†ï¼Œç”¨äºä¸Šä¸‹æ–‡åˆ†æ"""
    # å¦‚æœåŒ…å«SQLæŸ¥è¯¢æ ¼å¼ï¼Œæå–SQLåé¢çš„å†…å®¹
    if 'ğŸ” **SQLæŸ¥è¯¢**:' in full_response:
        # æ‰¾åˆ°SQLä»£ç å—ç»“æŸåçš„å†…å®¹
        parts = full_response.split('```')
        if len(parts) >= 3:
            # è¿”å›ç¬¬äºŒä¸ª```åé¢çš„å†…å®¹
            answer_part = parts[2].strip()
            # ç§»é™¤å¼€å¤´çš„æ¢è¡Œ
            return answer_part.lstrip('\n').strip()
    
    # å¦‚æœæ²¡æœ‰SQLæ ¼å¼ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
    return full_response

# å¤„ç†ä»£è¯å¼•ç”¨çš„è¾…åŠ©å‡½æ•°
def has_implicit_context_reference(question: str, conversation_history: list) -> bool:
    """æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«éšå¼çš„ä¸Šä¸‹æ–‡å¼•ç”¨"""
    if not conversation_history:
        return False
    
    question_lower = question.lower()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯
    context_indicators = [
        'å“ªä¸€å¹´', 'ä»€ä¹ˆæ—¶å€™', 'åœ¨å“ªé‡Œ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¤šå°‘',
        'æ˜¯å“ª', 'æ˜¯ä»€ä¹ˆ', 'æ˜¯è°', 'æ˜¯æ€ä¹ˆ', 'æœ‰å¤š', 'æœ‰ä»€ä¹ˆ'
    ]
    
    # å¦‚æœé—®é¢˜ä»¥è¿™äº›è¯å¼€å¤´ï¼Œä¸”é—®é¢˜å¾ˆçŸ­ï¼Œå¾ˆå¯èƒ½æ˜¯å¯¹ä¸Šä¸€ä¸ªç­”æ¡ˆçš„è¿½é—®
    for indicator in context_indicators:
        if question_lower.startswith(indicator) and len(question.strip()) < 30:
            return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç®€çŸ­çš„è¿½é—®ï¼ˆåªæœ‰å‡ ä¸ªå­—ï¼Œä¸”åŒ…å«ç–‘é—®è¯ï¼‰
    question_words = question.strip().split()
    if len(question_words) <= 8 and any(word in question_lower for word in ['ä»€ä¹ˆ', 'å“ª', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¤šå°‘', 'ä½•æ—¶']):
        return True
    
    return False

def has_pronoun_reference(question: str) -> bool:
    """æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«ä»£è¯å¼•ç”¨"""
    pronouns = ['ä»–ä»¬', 'å®ƒä»¬', 'è¿™ä¸ª', 'è¿™äº›', 'é‚£ä¸ª', 'é‚£äº›', 'ä»–', 'å¥¹', 'å®ƒ', 'å…¶', 'æ­¤']
    return any(pronoun in question for pronoun in pronouns)

def process_pronoun_references(question: str, conversation_history: list) -> str:
    """å¤„ç†é—®é¢˜ä¸­çš„ä»£è¯å¼•ç”¨å’Œéšå¼ä¸Šä¸‹æ–‡å¼•ç”¨ï¼ŒåŸºäºå¯¹è¯å†å²æ›¿æ¢ä»£è¯"""
    if not conversation_history:
        return question
    
    # è·å–æœ€è¿‘çš„å¯¹è¯
    if len(conversation_history) > 0:
        last_question, last_full_answer = conversation_history[-1]
        # æå–çº¯ç­”æ¡ˆéƒ¨åˆ†ï¼Œå»é™¤SQLä¿¡æ¯
        last_answer = extract_answer_from_full_response(last_full_answer)
        
        # ç®€å•çš„ä»£è¯æ›¿æ¢ç­–ç•¥
        processed_question = question
        
        # å¤„ç†æ˜æ˜¾çš„ä»£è¯å¼•ç”¨
        if 'ä»–ä»¬' in question and 'éƒ¨é—¨' in last_answer:
            # å°è¯•ä»ä¸Šä¸€ä¸ªå›ç­”ä¸­æå–éƒ¨é—¨åç§°
            import re
            dept_match = re.search(r'(\w+)éƒ¨é—¨', last_answer)
            if dept_match:
                dept_name = dept_match.group(1)
                processed_question = processed_question.replace('ä»–ä»¬', f'{dept_name}éƒ¨é—¨')
            elif 'å¹³å‡å·¥èµ„æœ€é«˜çš„éƒ¨é—¨' in last_answer or 'å¹³å‡è–ªèµ„æœ€é«˜' in last_answer:
                # å¦‚æœä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å…³äºå¹³å‡å·¥èµ„æœ€é«˜çš„éƒ¨é—¨
                processed_question = processed_question.replace('ä»–ä»¬', 'å¹³å‡å·¥èµ„æœ€é«˜çš„éƒ¨é—¨')
        
        # å¤„ç†å…¶ä»–ä»£è¯
        if 'å®ƒ' in question:
            # å°è¯•ä»ä¸Šä¸€ä¸ªå›ç­”ä¸­æå–å®ä½“
            import re
            entity_patterns = [
                r'(\w+å±)',  # åœ°è´¨å­¦ä¸­çš„å±
                r'(\w+æ ·å“)',  # æ ·å“
                r'(\w+çºª)',   # åœ°è´¨çºª
                r'(\w+åŒ–çŸ³)',  # åŒ–çŸ³
            ]
            for pattern in entity_patterns:
                match = re.search(pattern, last_answer)
                if match:
                    entity = match.group(1)
                    processed_question = processed_question.replace('å®ƒ', entity)
                    break
        
        # å¤„ç†éšå¼ä¸Šä¸‹æ–‡å¼•ç”¨
        if has_implicit_context_reference(question, conversation_history):
            # å¯¹äºéšå¼å¼•ç”¨ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            if 'å“ªä¸€å¹´' in question.lower() or 'ä»€ä¹ˆæ—¶å€™' in question.lower():
                # å¦‚æœé—®çš„æ˜¯æ—¶é—´ï¼Œå°è¯•æ‰¾åˆ°ä¸Šä¸€ä¸ªç­”æ¡ˆä¸­çš„ä¸»è¦å®ä½“
                import re
                # æ‰¾åˆ°åŒ–çŸ³åç§°
                fossil_match = re.search(r'([\w\s]+åŒ–çŸ³|[\w\s]+fossil)', last_answer, re.IGNORECASE)
                if fossil_match:
                    fossil_name = fossil_match.group(1)
                    processed_question = f"{fossil_name}{processed_question}"
                # æ‰¾åˆ°å…¶ä»–å®ä½“
                elif re.search(r'(\w+å±|æœ€æ—©çš„\w+)', last_answer):
                    entity_match = re.search(r'(æœ€æ—©çš„\w+|\w+å±|\w+æ ·å“)', last_answer)
                    if entity_match:
                        entity = entity_match.group(1)
                        processed_question = f"{entity}{processed_question}"
                        
            elif 'åœ¨å“ªé‡Œ' in question.lower() or 'å“ªä¸ªåœ°æ–¹' in question.lower():
                # å¦‚æœé—®çš„æ˜¯åœ°ç‚¹
                import re
                entity_match = re.search(r'(æœ€æ—©çš„\w+|\w+åŒ–çŸ³|\w+å±)', last_answer)
                if entity_match:
                    entity = entity_match.group(1)
                    processed_question = f"{entity}{processed_question}"
        
        # å¦‚æœé—®é¢˜åŒ…å«"è¿™ä¸ª"æˆ–"è¿™äº›"
        if 'è¿™ä¸ª' in question or 'è¿™äº›' in question:
            # åŸºäºä¸Šä¸€ä¸ªé—®é¢˜çš„ä¸»é¢˜æ¥æ¨æ–­
            if 'éƒ¨é—¨' in last_question:
                processed_question = processed_question.replace('è¿™ä¸ª', 'è¿™ä¸ªéƒ¨é—¨').replace('è¿™äº›', 'è¿™äº›éƒ¨é—¨')
            elif 'æ ·å“' in last_question or 'åŒ–çŸ³' in last_question:
                processed_question = processed_question.replace('è¿™ä¸ª', 'è¿™ä¸ªåŒ–çŸ³').replace('è¿™äº›', 'è¿™äº›åŒ–çŸ³')
    
    return processed_question

def generate_contextual_query(processed_question: str, conversation_history: list, session_info: dict) -> str:
    """åŸºäºä¸Šä¸‹æ–‡ç”ŸæˆSQLæŸ¥è¯¢"""
    if not conversation_history:
        return f"SELECT * FROM {session_info['table_name']} LIMIT 10"
    
    last_question, last_answer = conversation_history[-1]
    
    # å¦‚æœå½“å‰é—®é¢˜æ˜¯å…³äº"ä»–ä»¬çš„å¹³å‡å·¥èµ„"å¹¶ä¸”ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å…³äº"å¹³å‡å·¥èµ„æœ€é«˜çš„éƒ¨é—¨"
    if 'å¹³å‡å·¥èµ„' in processed_question and 'å¹³å‡å·¥èµ„æœ€é«˜' in last_answer:
        # ç”ŸæˆæŸ¥è¯¢æœ€é«˜å¹³å‡å·¥èµ„éƒ¨é—¨çš„å…·ä½“å·¥èµ„æ•°å€¼
        return f"SELECT department, AVG(salary) as avg_salary FROM {session_info['table_name']} GROUP BY department ORDER BY avg_salary DESC LIMIT 1"
    
    # é»˜è®¤æŸ¥è¯¢
    return f"SELECT * FROM {session_info['table_name']} LIMIT 10"

# æ™ºèƒ½åˆ†ææŸ¥è¯¢ç»“æœ
def analyze_with_llm(question: str, sql_query: str, result: list, llm, session_info: dict = None, conversation_history: list = None) -> str:
    """ä½¿ç”¨LLMåˆ†æSQLæŸ¥è¯¢ç»“æœï¼Œæä¾›æ™ºèƒ½è§£é‡Š"""
    if not result:
        return "æˆ‘ä¸çŸ¥é“"

    if not llm:
        return "æˆ‘ä¸çŸ¥é“"

    try:
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        context_info = ""
        if conversation_history:
            context_info = "\n\nå¯¹è¯å†å²ï¼š\n"
            for i, (prev_q, prev_full_a) in enumerate(conversation_history[-3:]):
                # æå–çº¯ç­”æ¡ˆéƒ¨åˆ†ï¼Œå»é™¤SQLä¿¡æ¯
                prev_a = extract_answer_from_full_response(prev_full_a)
                context_info += f"{i+1}. é—®é¢˜ï¼š{prev_q}\n   å›ç­”ï¼š{prev_a}\n"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³é—®é¢˜
            if has_implicit_context_reference(question, conversation_history) or has_pronoun_reference(question):
                context_info += "\né‡è¦æç¤ºï¼šå½“å‰é—®é¢˜ä¼¼ä¹ä¸ä¸Šä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆç›¸å…³ã€‚è¯·ä»”ç»†åˆ†æå¯¹è¯å†å²ï¼Œç†è§£å½“å‰é—®é¢˜æ‰€æŒ‡çš„å…·ä½“å¯¹è±¡æˆ–å®ä½“ã€‚\n"
            
            context_info += "\nè¯·æ ¹æ®å¯¹è¯å†å²ç†è§£å½“å‰é—®é¢˜ä¸­çš„ä»£è¯å’Œä¸Šä¸‹æ–‡å¼•ç”¨ã€‚\n"

        # å‡†å¤‡åˆ†ææç¤º
        context = f"""
ç”¨æˆ·é—®é¢˜ï¼š{question}

æ‰§è¡Œçš„SQLæŸ¥è¯¢ï¼š
{sql_query}

æŸ¥è¯¢ç»“æœï¼š
{result}{context_info}

è¯·åˆ†ææŸ¥è¯¢ç»“æœå¹¶å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦é‡å¤æŸ¥è¯¢ç»“æœçš„åŸå§‹æ•°æ®
2. å¦‚æœç”¨æˆ·ä½¿ç”¨äº†ä»£è¯ï¼ˆå¦‚"ä»–ä»¬"ã€"å®ƒ"ã€"è¿™ä¸ª"ç­‰ï¼‰ï¼Œè¯·ç»“åˆå¯¹è¯å†å²æ¥ç†è§£æŒ‡ä»£å†…å®¹
3. å¦‚æœæ¶‰åŠæ—¶é—´/å¹´ä»£æ¯”è¾ƒï¼Œè¯·æä¾›ä¸“ä¸šçš„æ—¶é—´é¡ºåºåˆ†æ
4. å¦‚æœæ¶‰åŠåœ°è´¨å¹´ä»£ï¼ˆå¦‚Silurian, Carboniferousç­‰ï¼‰ï¼Œè¯·è¯´æ˜å®ƒä»¬çš„æ—¶é—´å…³ç³»
5. å¦‚æœæ¶‰åŠåœ°è´¨æ¼”åŒ–ç¨‹åº¦ï¼Œè¯·è€ƒè™‘SiO2ã€MgOã€K2Oç­‰åŒ–å­¦æˆåˆ†çš„æ„ä¹‰
6. å¦‚æœæ˜¯æ•°å€¼æ¯”è¾ƒï¼Œè¯·æ˜ç¡®æŒ‡å‡ºæœ€å¤§/æœ€å°å€¼
7. å¦‚æœæ˜¯ç»Ÿè®¡åˆ†æï¼Œè¯·æä¾›æ¸…æ™°çš„æ€»ç»“
8. è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸š
9. å¦‚æœæ— æ³•å¾—å‡ºæ˜ç¡®ç»“è®ºï¼Œè¯·ç›´æ¥å›ç­”"æˆ‘ä¸çŸ¥é“"
"""

        # è°ƒç”¨LLMåˆ†æ
        from langchain_core.messages import HumanMessage
        messages = [HumanMessage(content=context)]
        response = llm.invoke(messages)

        if hasattr(response, 'content'):
            return response.content
        else:
            return str(response)

    except Exception as e:
        print(f"LLMåˆ†æå¤±è´¥: {e}")
        return "æˆ‘ä¸çŸ¥é“"

def format_answer(question: str, sql_query: str, result: list, table_name: str) -> str:
    """å°†SQLæŸ¥è¯¢ç»“æœè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€å›ç­”"""
    if not result:
        return "æˆ‘ä¸çŸ¥é“"

    question_lower = question.lower()

    # å¤„ç†è®¡æ•°ç±»é—®é¢˜
    if "å¤šå°‘" in question_lower or "æ•°é‡" in question_lower or "count" in question_lower:
        if len(result) == 1 and len(result[0]) == 1:
            count = list(result[0].values())[0]
            if "è¡Œ" in question_lower:
                return f"æ•°æ®æ€»å…±æœ‰ {count} è¡Œã€‚"
            elif "äºº" in question_lower:
                return f"å…±æœ‰ {count} äººã€‚"
            else:
                return f"æ€»æ•°é‡ä¸º {count}ã€‚"
    
    # å¤„ç†æ˜¾ç¤ºå‰Næ¡çš„é—®é¢˜
    if "å‰" in question_lower and ("æ¡" in question_lower or "è¡Œ" in question_lower):
        import re
        numbers = re.findall(r'\d+', question)
        limit = numbers[0] if numbers else "å‡ "
        
        # æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®
        if result:
            formatted_result = "ä»¥ä¸‹æ˜¯å‰{}æ¡è®°å½•ï¼š\n\n".format(limit)
            for i, row in enumerate(result, 1):
                formatted_result += f"{i}. "
                row_parts = []
                for key, value in row.items():
                    row_parts.append(f"{key}: {value}")
                formatted_result += ", ".join(row_parts) + "\n"
            return formatted_result
    
    # å¤„ç†æŸ¥æ‰¾æœ€å€¼çš„é—®é¢˜
    if "æœ€é«˜" in question_lower or "æœ€å¤§" in question_lower:
        if result:
            row = result[0]
            if "è–ªèµ„" in question_lower:
                name = row.get('å§“å', 'æœªçŸ¥')
                salary = row.get('è–ªèµ„', 'æœªçŸ¥')
                return f"è–ªèµ„æœ€é«˜çš„æ˜¯ {name}ï¼Œè–ªèµ„ä¸º {salary} å…ƒã€‚"
            else:
                return f"æœ€å¤§å€¼è®°å½•ï¼š{', '.join([f'{k}: {v}' for k, v in row.items()])}"
    
    if "æœ€ä½" in question_lower or "æœ€å°" in question_lower:
        if result:
            row = result[0]
            if "è–ªèµ„" in question_lower:
                name = row.get('å§“å', 'æœªçŸ¥')
                salary = row.get('è–ªèµ„', 'æœªçŸ¥')
                return f"è–ªèµ„æœ€ä½çš„æ˜¯ {name}ï¼Œè–ªèµ„ä¸º {salary} å…ƒã€‚"
            else:
                return f"æœ€å°å€¼è®°å½•ï¼š{', '.join([f'{k}: {v}' for k, v in row.items()])}"
    
    # å¤„ç†å¹³å‡å€¼é—®é¢˜
    if "å¹³å‡" in question_lower:
        if len(result) == 1 and len(result[0]) == 1:
            avg_value = list(result[0].values())[0]
            if "è–ªèµ„" in question_lower or "å·¥èµ„" in question_lower:
                return f"å¹³å‡è–ªèµ„ä¸º {avg_value:.2f} å…ƒã€‚"
            else:
                return f"å¹³å‡å€¼ä¸º {avg_value:.2f}ã€‚"
        elif len(result) == 1 and 'department' in result[0] and 'avg_salary' in result[0]:
            # å¤„ç†å•ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥èµ„æŸ¥è¯¢ç»“æœ
            dept = result[0]['department']
            avg_sal = result[0]['avg_salary']
            if "æœ€é«˜" in question_lower:
                return f"å¹³å‡å·¥èµ„æœ€é«˜çš„éƒ¨é—¨æ˜¯ {dept}ï¼Œå¹³å‡å·¥èµ„ä¸º {avg_sal:.2f} å…ƒã€‚"
            elif "æœ€ä½" in question_lower:
                return f"å¹³å‡å·¥èµ„æœ€ä½çš„éƒ¨é—¨æ˜¯ {dept}ï¼Œå¹³å‡å·¥èµ„ä¸º {avg_sal:.2f} å…ƒã€‚"
            else:
                return f"{dept} çš„å¹³å‡å·¥èµ„ä¸º {avg_sal:.2f} å…ƒã€‚"
    
    # å¤„ç†åˆ†ç»„ç»Ÿè®¡é—®é¢˜
    if "å„" in question_lower and ("éƒ¨é—¨" in question_lower or "åˆ†ç»„" in question_lower):
        if result:
            formatted_result = "ç»Ÿè®¡ç»“æœå¦‚ä¸‹ï¼š\n\n"
            for row in result:
                parts = []
                for key, value in row.items():
                    if "count" in key.lower() or "æ•°é‡" in key:
                        parts.append(f"{value}äºº")
                    elif "avg" in key.lower() or "å¹³å‡" in key:
                        parts.append(f"å¹³å‡{value:.2f}")
                    else:
                        parts.append(f"{key}: {value}")
                formatted_result += "â€¢ " + ", ".join(parts) + "\n"
            return formatted_result
    
    # å¤„ç†ç­›é€‰æ¡ä»¶çš„é—®é¢˜
    if "çš„" in question_lower and len(result) > 0:
        if len(result) == 1:
            row = result[0]
            formatted_result = "æŸ¥æ‰¾åˆ°1æ¡è®°å½•ï¼š\n"
            formatted_result += ", ".join([f"{k}: {v}" for k, v in row.items()])
            return formatted_result
        else:
            formatted_result = f"æŸ¥æ‰¾åˆ°{len(result)}æ¡è®°å½•ï¼š\n\n"
            for i, row in enumerate(result[:10], 1):  # æœ€å¤šæ˜¾ç¤º10æ¡
                formatted_result += f"{i}. "
                formatted_result += ", ".join([f"{k}: {v}" for k, v in row.items()])
                formatted_result += "\n"
            if len(result) > 10:
                formatted_result += f"... è¿˜æœ‰{len(result) - 10}æ¡è®°å½•"
            return formatted_result
    
    # é»˜è®¤æ ¼å¼åŒ–
    if len(result) == 1 and len(result[0]) == 1:
        # å•ä¸ªå€¼ç»“æœ
        value = list(result[0].values())[0]
        return f"æŸ¥è¯¢ç»“æœï¼š{value}"
    elif len(result) <= 5:
        # å°‘é‡è®°å½•ï¼Œå®Œæ•´æ˜¾ç¤º
        formatted_result = f"æŸ¥è¯¢åˆ°{len(result)}æ¡è®°å½•ï¼š\n\n"
        for i, row in enumerate(result, 1):
            formatted_result += f"{i}. "
            formatted_result += ", ".join([f"{k}: {v}" for k, v in row.items()])
            formatted_result += "\n"
        return formatted_result
    else:
        # å¤§é‡è®°å½•ï¼Œåªæ˜¾ç¤ºå‰å‡ æ¡
        formatted_result = f"æŸ¥è¯¢åˆ°{len(result)}æ¡è®°å½•ï¼Œæ˜¾ç¤ºå‰5æ¡ï¼š\n\n"
        for i, row in enumerate(result[:5], 1):
            formatted_result += f"{i}. "
            formatted_result += ", ".join([f"{k}: {v}" for k, v in row.items()])
            formatted_result += "\n"
        formatted_result += f"... è¿˜æœ‰{len(result) - 5}æ¡è®°å½•"
        return formatted_result

def create_full_response(question: str, sql_query: str, result: list, session_info: dict, conversation_history: list) -> str:
    """åˆ›å»ºåŒ…å«SQLæŸ¥è¯¢å’Œç­”æ¡ˆçš„å®Œæ•´å“åº”"""
    # æ¸…ç†SQLæŸ¥è¯¢ï¼Œå»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    clean_sql = ' '.join(sql_query.split())
    
    print(f"[DEBUG] æ­£åœ¨åˆ›å»ºåŒ…å«SQLçš„å“åº”: {clean_sql[:50]}...")  # è°ƒè¯•ä¿¡æ¯
    
    # åˆ›å»ºLLMå®ä¾‹è¿›è¡Œæ™ºèƒ½åˆ†æ
    llm = create_llm()
    
    if llm:
        # å°è¯•LLMæ™ºèƒ½åˆ†æ
        llm_analysis = analyze_with_llm(question, sql_query, result, llm, session_info, conversation_history)
        if llm_analysis and llm_analysis != "æˆ‘ä¸çŸ¥é“":
            final_response = f"ğŸ” **SQLæŸ¥è¯¢**: ```sql\n{clean_sql}\n```\n\n{llm_analysis}"
            print(f"[DEBUG] LLMåˆ†ææˆåŠŸï¼Œè¿”å›åŒ…å«SQLçš„å“åº”")  # è°ƒè¯•ä¿¡æ¯
            return final_response
    
    # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–
    basic_answer = format_answer(question, sql_query, result, session_info['table_name'])
    final_response = f"ğŸ” **SQLæŸ¥è¯¢**: ```sql\n{clean_sql}\n```\n\n{basic_answer}"
    print(f"[DEBUG] ä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–ï¼Œè¿”å›åŒ…å«SQLçš„å“åº”")  # è°ƒè¯•ä¿¡æ¯
    return final_response

# æœ¬åœ°æ‰§è¡Œ SQLï¼ˆä¸ä¾èµ– LangChainï¼‰
def execute_sql(db_path: str, sql_query: str):
    try:
        conn = sqlite3.connect(db_path)
        try:
            df = pd.read_sql_query(sql_query, conn)
            # å¤„ç†NaNå€¼ï¼Œç¡®ä¿JSONåºåˆ—åŒ–ä¸å‡ºé”™
            df = df.fillna('')
            return df.to_dict('records')
        finally:
            conn.close()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"SQLæ‰§è¡Œå¤±è´¥: {str(e)}")

@app.get("/")
async def root():
    return {"message": "æ™ºèƒ½æ•°æ®é—®ç­”ç³»ç»Ÿ API"}

@app.post("/upload-file")
async def upload_file(file: UploadFile = File(...)):
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆ100MBé™åˆ¶ï¼‰
        if hasattr(file, 'size') and file.size and file.size > 100 * 1024 * 1024:
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡100MBé™åˆ¶")
        
        if not file.filename.endswith(('.csv', '.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ CSVæˆ–Excelæ–‡ä»¶")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        file_path = os.path.join(temp_dir, file.filename)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ - æ”¯æŒå¤§æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            # åˆ†å—è¯»å–ä»¥æ”¯æŒå¤§æ–‡ä»¶
            chunk_size = 8192  # 8KB chunks
            while True:
                chunk = await file.read(chunk_size)
                if not chunk:
                    break
                buffer.write(chunk)
        
        # è¯»å–æ•°æ®
        _, ext = os.path.splitext(file.filename)
        if ext.lower() == '.csv':
            df = read_csv_with_auto_encoding(file_path)
        else:
            df = read_excel_with_engine(file_path, ext)
        
        # æ•°æ®æ¸…ç†ï¼šå¤„ç†Excelæ–‡ä»¶å¸¸è§é—®é¢˜
        # 1. æ¸…ç†åˆ—åä¸­çš„NULLå­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦
        df.columns = [str(col).replace('\x00', '').replace('\n', '_').replace('\r', '_').strip() 
                      if col is not None else f'æœªå‘½ååˆ—_{i}' 
                      for i, col in enumerate(df.columns)]
        
        # 2. å¤„ç†é‡å¤åˆ—å
        seen_columns = {}
        new_columns = []
        for col in df.columns:
            if col in seen_columns:
                seen_columns[col] += 1
                new_columns.append(f"{col}_{seen_columns[col]}")
            else:
                seen_columns[col] = 0
                new_columns.append(col)
        df.columns = new_columns
        
        # 3. å¤„ç†NaNå€¼å’Œæ— ç©·å¤§å€¼
        df = df.replace([float('inf'), float('-inf')], None)  # æ›¿æ¢æ— ç©·å¤§å€¼
        df = df.fillna('')  # ç”¨ç©ºå­—ç¬¦ä¸²æ›¿æ¢NaNå€¼
        
        # 4. ç¡®ä¿æ‰€æœ‰åˆ—åéƒ½æ˜¯æœ‰æ•ˆçš„SQLiteæ ‡è¯†ç¬¦
        df.columns = [f"col_{i}" if not str(col).strip() or str(col).startswith(tuple('0123456789')) 
                      else str(col).replace(' ', '_').replace('-', '_').replace('.', '_')
                      for i, col in enumerate(df.columns)]
        
        # åˆ›å»ºSQLiteæ•°æ®åº“
        session_id = f"session_{len(sessions)}"
        db_path = os.path.join(temp_dir, f"{session_id}.db")
        
        # å°†æ•°æ®å­˜å…¥SQLite
        conn = sqlite3.connect(db_path)
        table_name = "data_table"
        df.to_sql(table_name, conn, index=False, if_exists='replace')
        conn.close()
        
        # å­˜å‚¨ä¼šè¯ä¿¡æ¯
        import time
        sessions[session_id] = {
            "db_path": db_path,
            "table_name": table_name,
            "file_name": file.filename,
            "columns": df.columns.tolist(),
            "row_count": len(df),
            "temp_dir": temp_dir,
            "created_at": time.time()
        }
        
        # ä¸ºå‰ç«¯è¿”å›çš„ç¤ºä¾‹æ•°æ®ä¹Ÿéœ€è¦æ¸…ç†NaNå€¼
        sample_data = df.head(3).fillna('').to_dict('records')
        
        return {
            "session_id": session_id,
            "data_info": {
                "file_name": file.filename,
                "columns": df.columns.tolist(),
                "row_count": len(df),
                "sample_data": sample_data
            }
        }
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/query")
async def query_data(request: QueryRequest):
    try:
        if request.session_id not in sessions:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_info = sessions[request.session_id]
        
        # è·å–æˆ–åˆ›å»ºå¯¹è¯å†å²
        conversation_id = request.conversation_id or f"conv_{request.session_id}_default"
        if conversation_id not in conversations:
            conversations[conversation_id] = []
        
        conversation_history = conversations[conversation_id]
        
        # å¤„ç†ä»£è¯å¼•ç”¨é—®é¢˜
        processed_question = process_pronoun_references(request.question, conversation_history)
        
        # åˆ›å»ºLLMå®ä¾‹
        llm = create_llm()
        
        # ç”ŸæˆSQLæŸ¥è¯¢
        sql_query = None
        
        if llm:
            try:
                # ä½¿ç”¨LangChainåˆ›å»ºSQLæŸ¥è¯¢é“¾ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
                from langchain_community.utilities import SQLDatabase
                from langchain.chains import create_sql_query_chain

                # åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆä¾› LangChain è¯»å–åº“ç»“æ„ï¼‰
                db = SQLDatabase.from_uri(f"sqlite:///{session_info['db_path']}")
                chain = create_sql_query_chain(llm, db)
                
                # ç”ŸæˆSQLæŸ¥è¯¢ï¼ˆä½¿ç”¨å¤„ç†åçš„é—®é¢˜ï¼‰
                sql_query = chain.invoke({"question": processed_question})
                
                # æ¸…ç†SQLæŸ¥è¯¢å­—ç¬¦ä¸²
                if isinstance(sql_query, str):
                    # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
                    sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
                    # æå–SQLè¯­å¥ï¼šæ‰¾åˆ°SELECTå¼€å§‹åˆ°åˆ†å·ç»“æŸçš„éƒ¨åˆ†
                    lines = sql_query.split('\n')
                    sql_lines = []
                    in_sql = False
                    for line in lines:
                        line = line.strip()
                        if line.upper().startswith(('SELECT', 'WITH')):
                            in_sql = True
                            sql_lines.append(line)
                        elif in_sql:
                            if line.endswith(';'):
                                sql_lines.append(line)
                                break
                            elif line and not line.startswith('--'):  # å¿½ç•¥æ³¨é‡Šè¡Œ
                                sql_lines.append(line)

                    if sql_lines:
                        sql_query = ' '.join(sql_lines)
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°SQLï¼Œä½¿ç”¨åŸºç¡€æŸ¥è¯¢
                        sql_query = f"SELECT * FROM {session_info['table_name']} LIMIT 10"
                        
            except Exception as llm_error:
                print(f"LLMæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {llm_error}")
                sql_query = None
        
        # å¦‚æœLLMç”ŸæˆSQLå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æŸ¥è¯¢é€»è¾‘
        if not sql_query:
            question_lower = request.question.lower()
            
            # æ”¹è¿›çš„åŸºç¡€æŸ¥è¯¢é€»è¾‘
            if "å¹³å‡" in question_lower and ("å·¥èµ„" in question_lower or "è–ªèµ„" in question_lower):
                if "éƒ¨é—¨" in question_lower:
                    if "æœ€é«˜" in question_lower or "æœ€å¤§" in question_lower:
                        sql_query = f"SELECT department, AVG(salary) as avg_salary FROM {session_info['table_name']} GROUP BY department ORDER BY avg_salary DESC LIMIT 1"
                    elif "æœ€ä½" in question_lower or "æœ€å°" in question_lower:
                        sql_query = f"SELECT department, AVG(salary) as avg_salary FROM {session_info['table_name']} GROUP BY department ORDER BY avg_salary ASC LIMIT 1"
                    else:
                        sql_query = f"SELECT department, AVG(salary) as avg_salary FROM {session_info['table_name']} GROUP BY department ORDER BY avg_salary DESC"
                else:
                    sql_query = f"SELECT AVG(salary) as avg_salary FROM {session_info['table_name']}"
            elif "åŒ–çŸ³" in question_lower or "å¹´ä»£" in question_lower or "æ—¶é—´" in question_lower:
                sql_query = f"SELECT * FROM {session_info['table_name']}"
            elif "æœ€é«˜" in question_lower and ("å·¥èµ„" in question_lower or "è–ªèµ„" in question_lower):
                if "éƒ¨é—¨" in question_lower:
                    sql_query = f"SELECT department, MAX(salary) as max_salary FROM {session_info['table_name']} GROUP BY department ORDER BY max_salary DESC LIMIT 1"
                else:
                    sql_query = f"SELECT * FROM {session_info['table_name']} ORDER BY salary DESC LIMIT 1"
            elif "å¤šå°‘è¡Œ" in question_lower or "è¡Œæ•°" in question_lower or "count" in question_lower:
                if "éƒ¨é—¨" in question_lower:
                    sql_query = f"SELECT department, COUNT(*) as count FROM {session_info['table_name']} GROUP BY department"
                else:
                    sql_query = f"SELECT COUNT(*) as total_rows FROM {session_info['table_name']}"
            elif "å‰" in question_lower and ("æ¡" in question_lower or "è¡Œ" in question_lower):
                import re
                numbers = re.findall(r'\d+', request.question)
                limit = numbers[0] if numbers else "10"
                sql_query = f"SELECT * FROM {session_info['table_name']} LIMIT {limit}"
            elif "å„" in question_lower and "éƒ¨é—¨" in question_lower:
                sql_query = f"SELECT department, COUNT(*) as count, AVG(salary) as avg_salary FROM {session_info['table_name']} GROUP BY department"
            elif "æ‰€æœ‰" in question_lower or "å…¨éƒ¨" in question_lower:
                sql_query = f"SELECT * FROM {session_info['table_name']}"
            else:
                # å¯¹äºä»£è¯å¼•ç”¨æˆ–éšå¼ä¸Šä¸‹æ–‡å¼•ç”¨ï¼Œå°è¯•åŸºäºä¸Šä¸‹æ–‡ç”ŸæˆæŸ¥è¯¢
                if (has_pronoun_reference(request.question) or has_implicit_context_reference(request.question, conversation_history)) and conversation_history:
                    sql_query = generate_contextual_query(processed_question, conversation_history, session_info)
                else:
                    sql_query = f"SELECT * FROM {session_info['table_name']} LIMIT 20"
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = execute_sql(session_info['db_path'], sql_query)
        
        # åˆ›å»ºå®Œæ•´å“åº”ï¼ˆåŒ…å«SQLå’Œç­”æ¡ˆï¼‰
        full_answer = create_full_response(request.question, sql_query, result, session_info, conversation_history)
        
        # åˆ›å»ºåŸºç¡€ç­”æ¡ˆç”¨äºä¸Šä¸‹æ–‡åˆ†æï¼ˆä¸åŒ…å«SQLï¼Œé¿å…æ±¡æŸ“ä¸Šä¸‹æ–‡ï¼‰
        basic_answer = format_answer(request.question, sql_query, result, session_info['table_name'])
        
        # ä¿å­˜å¯¹è¯å†å²ï¼ˆä¿å­˜å®Œæ•´ç­”æ¡ˆç”¨äºæ˜¾ç¤ºï¼Œä½†åœ¨ä¸Šä¸‹æ–‡åˆ†ææ—¶ä¼šè¢«è¿‡æ»¤ï¼‰
        conversations[conversation_id].append((request.question, full_answer))
        if len(conversations[conversation_id]) > 10:
            conversations[conversation_id] = conversations[conversation_id][-10:]
        
        return {
            "question": request.question,
            "answer": full_answer,
            "success": True,
            "conversation_id": conversation_id,
            "note": "SQLæŸ¥è¯¢å·²æ˜¾ç¤º"
        }
        
    except Exception as e:
        return {
            "question": request.question,
            "answer": f"æŸ¥è¯¢å‡ºé”™ï¼š{str(e)}",
            "success": False
        }

@app.get("/sessions")
async def list_all_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯åˆ—è¡¨"""
    session_list = []
    for session_id, session_info in sessions.items():
        # è·å–è¯¥ä¼šè¯çš„å¯¹è¯æ•°é‡
        conversation_count = len([conv_id for conv_id in conversations.keys() if conv_id.startswith(f"conv_{session_id}_")])
        
        session_list.append({
            "session_id": session_id,
            "file_name": session_info["file_name"],
            "row_count": session_info["row_count"],
            "columns_count": len(session_info["columns"]),
            "conversation_count": conversation_count,
            "created_at": session_info.get("created_at", "Unknown")
        })
    
    # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    session_list.sort(key=lambda x: x.get('created_at', ''), reverse=True)
    
    return {
        "sessions": session_list
    }

@app.get("/sessions/{session_id}/conversations")
async def list_conversations(session_id: str):
    """è·å–ä¼šè¯çš„æ‰€æœ‰å¯¹è¯åˆ—è¡¨"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    # æŸ¥æ‰¾æ‰€æœ‰å±äºè¯¥sessionçš„å¯¹è¯
    session_conversations = []
    for conv_id, history in conversations.items():
        if conv_id.startswith(f"conv_{session_id}_"):
            # è®¡ç®—å¯¹è¯ä¿¡æ¯
            message_count = len(history)
            last_activity = None
            conversation_title = "æ–°å¯¹è¯"
            
            if history:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªé—®é¢˜ä½œä¸ºæ ‡é¢˜ï¼ˆæˆªå–å‰30ä¸ªå­—ç¬¦ï¼‰
                first_question = history[0][0]
                conversation_title = first_question[:30] + ("..." if len(first_question) > 30 else "")
                # æœ€åä¸€æ¬¡æ´»åŠ¨æ—¶é—´ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…åº”è¯¥ç”¨æ—¶é—´æˆ³ï¼‰
                import datetime
                last_activity = datetime.datetime.now().isoformat()
            
            session_conversations.append({
                "conversation_id": conv_id,
                "title": conversation_title,
                "message_count": message_count,
                "last_activity": last_activity
            })
    
    # æŒ‰æœ€åæ´»åŠ¨æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    session_conversations.sort(key=lambda x: x['last_activity'] or '', reverse=True)
    
    return {
        "session_id": session_id,
        "conversations": session_conversations
    }

@app.post("/sessions/{session_id}/conversations")
async def create_new_conversation(session_id: str):
    """ä¸ºæŒ‡å®šä¼šè¯åˆ›å»ºæ–°çš„å¯¹è¯"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    # ç”Ÿæˆæ–°çš„å¯¹è¯ ID
    import time
    new_conversation_id = f"conv_{session_id}_{int(time.time() * 1000)}"
    
    # åˆå§‹åŒ–ç©ºçš„å¯¹è¯å†å²
    conversations[new_conversation_id] = []
    
    return {
        "conversation_id": new_conversation_id,
        "title": "æ–°å¯¹è¯",
        "message_count": 0,
        "created_at": time.time()
    }

@app.get("/sessions/{session_id}/conversations/{conversation_id}")
async def get_conversation_history(session_id: str, conversation_id: str):
    """è·å–å¯¹è¯å†å²"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    if conversation_id not in conversations:
        return {"conversation_id": conversation_id, "history": []}
    
    return {
        "conversation_id": conversation_id,
        "history": conversations[conversation_id]
    }

@app.delete("/sessions/{session_id}/conversations/{conversation_id}")
async def clear_conversation_history(session_id: str, conversation_id: str):
    """æ¸…ç©ºå¯¹è¯å†å²"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    if conversation_id in conversations:
        conversations[conversation_id] = []
    
    return {"message": "å¯¹è¯å†å²å·²æ¸…ç©º"}

@app.get("/sessions/{session_id}/info")
async def get_session_info(session_id: str):
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    session_info = sessions[session_id]
    return {
        "session_id": session_id,
        "file_name": session_info["file_name"],
        "columns": session_info["columns"],
        "row_count": session_info["row_count"]
    }

@app.post("/config/model")
async def configure_model(config: ModelConfig):
    """é…ç½®å¤§æ¨¡å‹API"""
    try:
        # æµ‹è¯•é…ç½®æ˜¯å¦æœ‰æ•ˆ
        llm = create_llm(config.api_key, config.api_base, config.model_name)
        if llm is None:
            raise HTTPException(status_code=400, detail="æ¨¡å‹é…ç½®æ— æ•ˆ")
        
        # å­˜å‚¨åˆ°ç¯å¢ƒå˜é‡ï¼ˆä»…åœ¨å½“å‰ä¼šè¯ä¸­æœ‰æ•ˆï¼‰
        os.environ["OPENAI_API_KEY"] = config.api_key
        os.environ["OPENAI_API_BASE"] = config.api_base
        os.environ["MODEL_NAME"] = config.model_name
        
        return {
            "message": "æ¨¡å‹é…ç½®æˆåŠŸ",
            "api_base": config.api_base,
            "model_name": config.model_name
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"é…ç½®å¤±è´¥: {str(e)}")

@app.get("/config/model")
async def get_model_config():
    """è·å–å½“å‰æ¨¡å‹é…ç½®"""
    return {
        "api_base": os.getenv("OPENAI_API_BASE", "æœªé…ç½®"),
        "model_name": os.getenv("MODEL_NAME", "æœªé…ç½®"),
        "api_key_configured": bool(os.getenv("OPENAI_API_KEY"))
    }

@app.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    if session_id in sessions:
        session_info = sessions[session_id]
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        if os.path.exists(session_info["temp_dir"]):
            shutil.rmtree(session_info["temp_dir"])
        del sessions[session_id]
        return {"message": "ä¼šè¯å·²åˆ é™¤"}
    else:
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        # limit_max_size å‚æ•°åœ¨uvicorn.runä¸­ä¸å¯ç”¨ï¼Œéœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼è®¾ç½®
    )